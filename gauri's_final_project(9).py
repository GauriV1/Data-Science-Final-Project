# -*- coding: utf-8 -*-
"""Gauri's Final Project(9)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tzd9kp9nikMtWJXH1T28fOb5yUTRaKJH
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install -q streamlit plotly
#

"""# Final Project
# Predicting Stock Prices Using a Hybrid ARIMA–LSTM Model

For CS 215

Prof Wirfs Brock

Gauri Vaidya, Whitman College

## TLDR

Why did I want to mix them up?

We pair them because ARIMA nabs the linear trends and seasonality, leaving the “hard,” non-linear leftovers—then the LSTM zeroes in on those complex patterns. Splitting the work like this speeds up training and usually beats using either model on its own. (Atleast that was my logic before starting this exploration)


*Data Attributes*: Kaggle World Stock Prices Zip upload (for non-api inference)
Access here = https://www.kaggle.com/datasets/nelgiriyewithana/world-stock-prices-daily-updating
This is updated almost daily

Google Finance for data retrieval for streamlit interaction


Time series fields: Date, Open, High, Low, Close, Volume

*References*:

I would like to acknowledge the use of Stack Overflow and the numerous resources available via Google insights and code examples as well as the Google Colab Python Helpdesk, which helped me resolve technical challenges that came up during this project. Additionally, I did refer to some of the modules in datacamp for formating and data parsing.

Additionally, I would also like to acknowledge the following resources for inspiration for logic handling for main prediction model
1. https://www.datacamp.com/tutorial/lstm-python-stock-market
2. https://www.kaggle.com/code/bryanb/stock-prices-forecasting-with-lstm
3. https://blog.quantinsti.com/forecasting-stock-returns-using-arima-model/



*Key Methodology*:

Preprocessing: handle missing values, engineer features, construct train/test splits (cutoff: April 30)

Modeling: fit an ARIMA to capture linear trends, train an LSTM on ARIMA residuals for non-linear patterns

Evaluation & Deployment: compute Checking efficiency on post-April-30 data, deploy predictions via a Streamlit interface (usually used for stock/finance stuff) pulling live prices from Finnhub or Alpha Vantage or google finance.

## Part 1

Pre-Processing the historical data upto the 30th of April
"""

import pandas as pd

#Selecting the ten tickers through parsing and normalization
#Load the dataset
df = pd.read_csv('/content/World-Stock-Prices-Dataset.csv')

#Parse the Date column to timezone‐naive datetime
df['Date'] = pd.to_datetime(df['Date'], utc=True).dt.tz_convert(None)

#Filter for records on or after January 1, 2000
df = df[df['Date'] >= pd.Timestamp('2000-01-01')]

#Compute the total set of unique trading dates in that period
all_dates = df['Date'].dt.normalize().unique()
n_all = len(all_dates)

#Count, for each ticker, how many unique dates it appears on
ticker_counts = df.groupby('Ticker')['Date'].nunique()

#Select the first 10 tickers with at least 95% coverage of those dates
selected_tickers = ticker_counts[ticker_counts >= 0.95 * n_all].index.tolist()[:10]

print("Selected tickers:", selected_tickers)

from google.colab import drive
drive.mount('/content/drive')

from sklearn.preprocessing import MinMaxScaler

#Creating a new file so as to process things faster,
#Loading the raw dataset again

#Loading the original dataset
df = pd.read_csv('/content/World-Stock-Prices-Dataset.csv')

#Parsing the Date column to timezone-naive datetime
df['Date'] = pd.to_datetime(df['Date'], utc=True).dt.tz_convert(None)

#Defining selected tickers and cutoff date
selected_tickers = ['AAPL', 'ADBE', 'AEO', 'AMD', 'AMZN', 'AXP', 'CL', 'COST', 'CSCO', 'DIS']
cutoff = pd.Timestamp('2025-04-30')

#Filtering for those tickers and dates up to the cutoff
df_sel = df[
    df['Ticker'].isin(selected_tickers) &
    (df['Date'] <= cutoff)
].copy()

#Sorting and handle missing values per ticker
df_clean = (
    df_sel
    .sort_values(['Ticker', 'Date'])
    .groupby('Ticker', group_keys=False)
    .apply(lambda grp: grp.set_index('Date').ffill().bfill())
    .reset_index()
)

#Saving the cleaned data to a new CSV
output_path = '/content/selected_tickers_clean.csv'
df_clean.to_csv(output_path, index=False)

print(f"Cleaned data saved to: {output_path}")

"""## Part 2

Modelling to fit an ARIMA to capture linear trends, train an LSTM on ARIMA residuals for non-linear patterns for predicting prices from the 1st of May to the 5th of May.

Check in line comments for code explanations
"""

"""import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam


# 1) Load cleaned data

# Load the CSV you generated earlier, which has Date, Ticker, Close, etc.
df = pd.read_csv('/content/selected_tickers_clean.csv', parse_dates=['Date'])


# 2) Define parameters

tickers = ['AAPL','ADBE','AEO','AMD','AMZN','AXP','CL','COST','CSCO','DIS']
cutoff = pd.Timestamp('2025-04-30')                # last date for training
forecast_dates = pd.date_range('2025-05-01','2025-05-05',freq='D')
arima_order = (2,1,1)                              # fixed ARIMA(p,d,q)
window_size = 10                                   # LSTM uses 10-day residual windows
epochs = 50                                        # number of training epochs for LSTM
batch_size = 16                                    # batch size for LSTM training

# Prepare a list to collect each ticker’s forecasts
all_forecasts = []


# 3) Loop over each ticker

for ticker in tickers:
    # --- 3a) Subset ticker history, drop duplicate dates, reindex daily ---
    series = (
        df.loc[(df.Ticker == ticker) & (df.Date <= cutoff), ['Date','Close']]
          # If there are duplicate entries for the same Date, keep the last one
          .drop_duplicates(subset='Date', keep='last')
          .set_index('Date')      # make Date the index
          .sort_index()           # ensure the index is sorted
          .asfreq('D')            # reindex to calendar daily frequency (introduces NaNs)
    )

    # --- 3b) Fit ARIMA(2,1,1) to capture linear trends & seasonality ---
    arima_model = ARIMA(series['Close'], order=arima_order).fit()

    # --- 3c) Forecast next 5 days with the ARIMA model ---
    arima_forecast = arima_model.predict(
        start=forecast_dates[0],
        end=forecast_dates[-1]
    )

    # --- 3d) Compute in-sample residuals: actual minus ARIMA fitted ---
    fitted_vals = arima_model.fittedvalues
    residuals = series['Close'] - fitted_vals

    # --- 3e) Prepare residuals for LSTM input ---
    resid_clean = residuals.dropna().values.reshape(-1,1)
    scaler = MinMaxScaler(feature_range=(0,1))
    resid_scaled = scaler.fit_transform(resid_clean)

    # Build sliding windows of size `window_size`
    X, y = [], []
    for i in range(window_size, len(resid_scaled)):
        X.append(resid_scaled[i-window_size:i, 0])
        y.append(resid_scaled[i, 0])
    X, y = np.array(X), np.array(y)
    X = X.reshape((X.shape[0], X.shape[1], 1))  # [samples, timesteps, features]

    # --- 3f) Define & train the LSTM model on residuals ---
    lstm_model = Sequential([
        LSTM(32, input_shape=(window_size, 1)),
        Dense(1)
    ])
    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    lstm_model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)

    # --- 3g) Forecast next 5 residuals using a rolling window ---
    last_window = resid_scaled[-window_size:].reshape(1, window_size, 1)
    lstm_preds = []
    for _ in forecast_dates:
        pred = lstm_model.predict(last_window, verbose=0)[0,0]
        lstm_preds.append(pred)
        # roll the window forward by dropping the oldest and appending the new pred
        last_window = np.roll(last_window, -1, axis=1)
        last_window[0, -1, 0] = pred

    # Transform scaled residual forecasts back to original units
    lstm_resid_forecast = scaler.inverse_transform(
        np.array(lstm_preds).reshape(-1,1)
    ).flatten()

    # --- 3h) Combine ARIMA forecast + LSTM residual forecast ---
    combined = arima_forecast.values + lstm_resid_forecast

    # --- 3i) Collect forecasts in a DataFrame ---
    temp_df = pd.DataFrame({
        'Ticker': ticker,
        'Date': forecast_dates,
        'ARIMA_forecast': arima_forecast.values,
        'LSTM_residual_forecast': lstm_resid_forecast,
        'Combined_forecast': combined
    })
    all_forecasts.append(temp_df)


# 4) Aggregate all tickers’ forecasts

forecast_df = pd.concat(all_forecasts, ignore_index=True)


# 5) Save or display the final forecasts

forecast_df.to_csv('/content/may1_5_forecasts.csv', index=False)
print(forecast_df.head(10))""" # Huge efficiency differences so we tried again

import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam


#Load cleaned data (6-year history)

df = pd.read_csv('/content/selected_tickers_clean.csv', parse_dates=['Date'])


#Define parameters
tickers       = ['AAPL','ADBE','AEO','AMD','AMZN','AXP','CL','COST','CSCO','DIS']
cutoff        = pd.Timestamp('2025-04-30')                   # end of training
start_date    = cutoff - pd.DateOffset(years=6)              # start = 6 years earlier :contentReference[oaicite:6]{index=6}
forecast_dates= pd.date_range('2025-05-01','2025-05-05',freq='D')

arima_order   = (3,1,5)                                      # ARIMA(p,d,q)= (3,1,5) best fit: check the citation below
window_size   = 20                                           # LSTM look-back of 20 days :contentReference
epochs        = 50
batch_size    = 16

all_forecasts = []  # collect results


#Loop over tickers

for ticker in tickers:
    # Subset 6-year history, drop duplicates, reindex daily
    series = (
        df.loc[
            (df.Ticker == ticker) &
            (df.Date >= start_date) &
            (df.Date <= cutoff),
            ['Date','Close']
        ]
        .drop_duplicates(subset='Date', keep='last')
        .set_index('Date')
        .sort_index()
        .asfreq('D')                                          # ensures daily calendar index :contentReference[oaicite:9]{index=9}
    )

    #Fit ARIMA(3,1,5) to model linear trends & lags
    arima_model      = ARIMA(series['Close'], order=arima_order).fit()

    # Generate 5-day ARIMA forecast
    arima_forecast   = arima_model.predict(
                          start=forecast_dates[0],
                          end=forecast_dates[-1]
                      )

    #Compute residuals (actual – fitted)
    residuals        = series['Close'] - arima_model.fittedvalues

    #Prepare residuals for LSTM
    resid_vals       = residuals.dropna().values.reshape(-1,1)
    scaler           = MinMaxScaler((0,1))
    scaled_resid     = scaler.fit_transform(resid_vals)

    # Build X, y using a 20-day sliding window
    X, y = [], []
    for i in range(window_size, len(scaled_resid)):
        X.append(scaled_resid[i-window_size:i, 0])
        y.append(scaled_resid[i, 0])
    X = np.array(X).reshape(-1, window_size, 1)
    y = np.array(y)

    #Train a simple LSTM on the residual series
    lstm_model = Sequential([
        LSTM(32, input_shape=(window_size, 1)),
        Dense(1)
    ])
    lstm_model.compile(optimizer=Adam(0.001), loss='mse')
    lstm_model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)

    #Rolling forecast of next 5 residuals
    last_window = scaled_resid[-window_size:].reshape(1, window_size, 1)
    lstm_preds  = []
    for _ in forecast_dates:
        p = lstm_model.predict(last_window, verbose=0)[0,0]
        lstm_preds.append(p)
        last_window = np.roll(last_window, -1, axis=1)
        last_window[0, -1, 0] = p

    #scaling back to original residual units
    lstm_resid_forecast = scaler.inverse_transform(
                             np.array(lstm_preds).reshape(-1,1)
                          ).flatten()

    #Combine ARIMA + LSTM residual predictions
    combined = arima_forecast.values + lstm_resid_forecast

    #Collect into DataFrame
    temp_df  = pd.DataFrame({
        'Ticker': ticker,
        'Date': forecast_dates,
        'ARIMA_forecast': arima_forecast.values,
        'LSTM_residual_forecast': lstm_resid_forecast,
        'Combined_forecast': combined
    })
    all_forecasts.append(temp_df)


#Aggregate & save

forecast_df = pd.concat(all_forecasts, ignore_index=True)
forecast_df.to_csv('/content/may1_5_forecasts_v2.csv', index=False)

print(forecast_df.head(10))

"""**Citations for the changes that were made**

Mashadihasanli’s 2022 application to the Istanbul stock index
https://dergipark.org.tr/en/download/article-file/2187949

We stretched our history to six years (May 2019–Apr 2025), not 25 years, so the models see the right amount market cycles and don’t over-fit on blips—longer history => usually means smoother, more reliable forecasts

Switched ARIMA from (2,1,1) to (3,1,5) because peer studies (e.g., on the Istanbul index above) show a (3,1,5) setup often beats simpler specs at capturing nuanced autocorrelations in stock data
.

Bumped the LSTM look-back from 2 days up to 20 days, since most LSTM tutorials and experiments (e.g. using a 21-day window) report that ~3–4 weeks of past residuals strikes the best balance between context and market noise.

## Part 3

## Deployment on streamlit and visualizations

While working on this part I hit the y-finance request limit. So I had to manually create a dataset from google finance using the sheets finance function  and upload it. (/content/Actual - Sheet1.csv)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import plotly.express as px
# import plotly.graph_objects as go
# from statsmodels.tsa.arima.model import ARIMA
# from datetime import timedelta
# 
# 
# 
# st.set_page_config(page_title="Gauri's Final DS Project", layout="wide")
# st.title("Hybrid ARIMA + LSTM Stock Forecast Dashboard")
# 
# 
# @st.cache_data
# def load_cleaned():
#     """Load cleaned historical OHLC data."""
#     return pd.read_csv(
#         '/content/selected_tickers_clean.csv',
#         parse_dates=['Date']
#     )
# 
# @st.cache_data
# def load_forecasts():
#     """Load combined ARIMA+LSTM forecasts for May 1–5, 2025."""
#     return pd.read_csv(
#         '/content/may1_5_forecasts_v2.csv',
#         parse_dates=['Date']
#     )
# 
# @st.cache_data
# def load_actual():
#     """
#     Load actual May 1–5, 2025 closes from wide-format sheet
#     and melt into long form.
#     """
#     w = pd.read_csv('/content/Actual - Sheet1.csv')
#     date_cols = [c for c in w.columns if c != 'Ticker']
#     df = w.melt(
#         id_vars=['Ticker'],
#         value_vars=date_cols,
#         var_name='Date',
#         value_name='Actual'
#     )
#     df['Date'] = pd.to_datetime(df['Date'])
#     return df[['Ticker','Date','Actual']]
# 
# clean_df     = load_cleaned()
# forecasts_df = load_forecasts()
# actual_df    = load_actual()
# 
# tickers = sorted(clean_df['Ticker'].unique())
# 
# -
# selected_ticker = st.sidebar.selectbox("Select Ticker", tickers)
# 
# # Precompute date bounds
# today      = clean_df['Date'].max()
# hist_start = today - timedelta(days=365*6)
# forecast_start = pd.Timestamp('2025-05-01')
# forecast_end   = pd.Timestamp('2025-05-05')
# 
# #tabs
# tab1, tab2, tab3, tab4 = st.tabs([
#     "📈 6-Year Trend",
#     "🔮 Forecast vs Actual",
#     "⚙️ Model Efficiency",
#     "📊 Train/Test Comparison"
# ])
# 
# 
# with tab1:
#     st.subheader(f"{selected_ticker} — Last 6 Years Close Price")
#     hist = clean_df[
#         (clean_df['Ticker'] == selected_ticker) &
#         (clean_df['Date'] >= hist_start)
#     ]
#     fig = px.line(
#         hist,
#         x='Date',
#         y='Close',
#         title=f"{selected_ticker} Close Price (Last 6 Years)",
#         labels={'Close':'Price','Date':'Date'}
#     )
#     fig.update_layout(xaxis_rangeslider_visible=True)
#     st.plotly_chart(fig, use_container_width=True)
# 
# 
# with tab2:
#     st.subheader(f"Forecast vs Actual (May 1–5, 2025) — {selected_ticker}")
#     # slice forecast & actual
#     fc = forecasts_df[forecasts_df['Ticker'] == selected_ticker]
#     ac = actual_df   [actual_df   ['Ticker'] == selected_ticker]
#     # merge on Date
#     merged = pd.merge(fc, ac, on='Date', how='inner')
#     # plot overlay
#     fig2 = go.Figure()
#     fig2.add_trace(go.Scatter(
#         x=merged['Date'], y=merged['Combined_forecast'],
#         mode='lines+markers', name='Forecast'
#     ))
#     fig2.add_trace(go.Bar(
#         x=merged['Date'], y=merged['Actual'],
#         name='Actual', opacity=0.6
#     ))
#     fig2.update_layout(
#         title="Forecast vs Actual",
#         xaxis_title="Date",
#         yaxis_title="Price",
#         xaxis_rangeslider_visible=True
#     )
#     st.plotly_chart(fig2, use_container_width=True)
# 
# 
# with tab3:
#     st.subheader(f"Model Efficiency — {selected_ticker}")
#     # re-merge to ensure local context
#     fc   = forecasts_df[forecasts_df['Ticker'] == selected_ticker]
#     ac   = actual_df   [actual_df   ['Ticker'] == selected_ticker]
#     merged = pd.merge(fc, ac, on='Date', how='inner')
#     # compute efficiency
#     merged['Efficiency'] = 1 - (merged['Combined_forecast'] - merged['Actual']).abs() / merged['Actual']
#     avg_eff = merged['Efficiency'].mean()
#     # gauge
#     gauge = go.Figure(go.Indicator(
#         mode="gauge+number",
#         value=avg_eff * 100,
#         number={'suffix': "%"},
#         gauge={'axis': {'range': [0,100]}},
#         title={'text': "Avg Daily Efficiency"}
#     ))
#     st.plotly_chart(gauge, use_container_width=True)
#     # daily line
#     eff_fig = go.Figure(go.Scatter(
#         x=merged['Date'], y=merged['Efficiency']*100,
#         mode='lines+markers', name='Efficiency (%)'
#     ))
#     eff_fig.update_layout(
#         title="Daily Efficiency (%)",
#         yaxis={'title': 'Efficiency (%)'}
#     )
#     st.plotly_chart(eff_fig, use_container_width=True)
# 
# 
# 
# with tab4:
#     st.subheader(f"Train vs Test Efficiency — {selected_ticker}")
#     # in-sample train
#     hist_train = clean_df[
#         (clean_df['Ticker'] == selected_ticker) &
#         (clean_df['Date'] < forecast_start)
#     ]
#     arima_model = ARIMA(hist_train['Close'], order=(3,1,5)).fit()
#     resid       = hist_train['Close'] - arima_model.fittedvalues
#     train_eff   = (1 - resid.abs() / hist_train['Close']).mean()
#     # out-of-sample test = avg from tab3
#     test_eff    = avg_eff
#     df_eff = pd.DataFrame({
#         'Period': ['Train','Test'],
#         'Efficiency (%)': [train_eff * 100, test_eff * 100]
#     })
#     fig3 = go.Figure(go.Bar(
#         x=df_eff['Period'],
#         y=df_eff['Efficiency (%)'],
#         text=df_eff['Efficiency (%)'].round(2),
#         textposition='auto'
#     ))
#     fig3.update_layout(
#         title="Train vs Test Efficiency (%)",
#         yaxis={'range': [0,100]}
#     )
#     st.plotly_chart(fig3, use_container_width=True)
# 
# # -----------------------------------------------------------------------------
# # Footer
# # -----------------------------------------------------------------------------
# st.markdown("---")
# st.markdown(
#     "**App:** Gauri's Final Data Science Project  |  "
#     "Hybrid ARIMA(3,1,5) + LSTM (20-day)  |  6-year history"
# )
#

#Visualizations
import pandas as pd
import matplotlib.pyplot as plt


#Load & prepare data

# Forecasts tidy: ['Ticker','Date','Combined_forecast',…]
forecasts_df = pd.read_csv(
    '/content/may1_5_forecasts_v2.csv',
    parse_dates=['Date']
)

# Actuals wide → melt to long: ['Ticker','Date','Actual']
actual_wide = pd.read_csv('/content/Actual - Sheet1.csv')
date_cols   = [c for c in actual_wide.columns if c != 'Ticker']
actual_df   = actual_wide.melt(
    id_vars='Ticker',
    value_vars=date_cols,
    var_name='Date',
    value_name='Actual'
)
actual_df['Date'] = pd.to_datetime(actual_df['Date'])

# Merge & drop missing
merged = (
    forecasts_df
    .merge(actual_df, on=['Ticker','Date'], how='inner')
    .dropna(subset=['Combined_forecast','Actual'])
)


# Plot 1: Predicted Prices for AAPL & AMZN only

# Pivot all forecasts
pred = merged.pivot(
    index='Date',
    columns='Ticker',
    values='Combined_forecast'
)

# Select only Apple & Amazon
to_plot = ['AAPL','AMZN']
pred_sub = pred[to_plot]

plt.figure(figsize=(8,5))
for ticker in to_plot:
    plt.plot(pred_sub.index, pred_sub[ticker], marker='o', label=ticker)
plt.title('Predicted Prices (May 1–5, 2025) for AAPL & AMZN')
plt.xlabel('Date')
plt.ylabel('Predicted Price')
# Tightly fit y-axis around these two
ymin = pred_sub.min().min() * 0.95
ymax = pred_sub.max().max() * 1.05
plt.ylim(ymin, ymax)
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()


# Actual vs Predicted on May 5, 2025 (all tickers)

focus_date = pd.Timestamp('2025-05-05')
sub = merged[merged['Date']==focus_date].reset_index(drop=True)

x = range(len(sub))
plt.figure(figsize=(8,5))
plt.bar([i-0.2 for i in x], sub['Actual'], width=0.4, label='Actual')
plt.bar([i+0.2 for i in x], sub['Combined_forecast'], width=0.4, label='Predicted')
plt.xticks(x, sub['Ticker'], rotation=45)
all_vals = pd.concat([sub['Actual'], sub['Combined_forecast']])
plt.ylim(all_vals.min()*0.95, all_vals.max()*1.05)
plt.title('Actual vs Predicted Prices on 2025-05-05')
plt.xlabel('Ticker')
plt.ylabel('Price')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()


# Plot 3: Efficiency Heatmap (May 1–5, 2025)

merged['Efficiency'] = 1 - (merged['Combined_forecast'] - merged['Actual']).abs()/merged['Actual']
heat = merged.pivot(index='Ticker', columns='Date', values='Efficiency')

plt.figure(figsize=(8,6))
plt.imshow(heat, aspect='auto', cmap='viridis', vmin=0, vmax=1)
plt.colorbar(label='Efficiency')
plt.xticks(range(len(heat.columns)), [d.strftime('%m-%d') for d in heat.columns], rotation=45)
plt.yticks(range(len(heat.index)), heat.index)
plt.title('Model Efficiency Heatmap (May 1–5, 2025)')
plt.tight_layout()
plt.show()

# Snapshot - Side by side comparison on a 0-1 scale
# Not normalizing it, dividing by hundred for better view-ability!

import pandas as pd
import matplotlib.pyplot as plt


# Forecasts tidy: ['Ticker','Date','Combined_forecast',…]
forecasts_df = pd.read_csv(
    '/content/may1_5_forecasts_v2.csv',
    parse_dates=['Date']
)

# Actuals wide → melt to long: ['Ticker','Date','Actual']
actual_wide = pd.read_csv('/content/Actual - Sheet1.csv')
date_cols   = [c for c in actual_wide.columns if c != 'Ticker']
actual_df   = actual_wide.melt(
    id_vars='Ticker',
    value_vars=date_cols,
    var_name='Date',
    value_name='Actual'
)
actual_df['Date'] = pd.to_datetime(actual_df['Date'])

# Merge & drop any missing
merged = (
    forecasts_df
    .merge(actual_df, on=['Ticker','Date'], how='inner')
    .dropna(subset=['Combined_forecast','Actual'])
)


tickers = merged['Ticker'].unique()
cmap    = plt.get_cmap('tab10', len(tickers))

plt.figure(figsize=(12, 6))
for i, ticker in enumerate(tickers):
    df_t = merged[merged['Ticker'] == ticker].sort_values('Date')
    # scale by dividing by 100 so $200 → 2.0
    scaled_actual = df_t['Actual'] / 100.0
    scaled_pred   = df_t['Combined_forecast'] / 100.0
    color = cmap(i)
    # plot actual as solid, predicted as dashed
    plt.plot(df_t['Date'], scaled_actual,   color=color, linestyle='-', label=f'{ticker} Actual')
    plt.plot(df_t['Date'], scaled_pred,     color=color, linestyle='--', label=f'{ticker} Pred')

plt.title('Actual vs Predicted Prices (Scaled ÷100) for All Tickers\n(May 1–5, 2025)')
plt.xlabel('Date')
plt.ylabel('Price ÷ 100')
plt.ylim(merged[['Actual','Combined_forecast']].min().min() / 100 * 0.95,
         merged[['Actual','Combined_forecast']].max().max() / 100 * 1.05)
plt.legend(fontsize='small', ncol=2, loc='upper left')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()